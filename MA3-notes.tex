\documentclass{tufte-handout}

\title{Notizen MA3 WS 22/23}

\author[Noah Freising]{Noah Freising}

%\date{28 March 2010} % without \date command, current date is supplied

%\geometry{showframe} % display margins for debugging page layout

\usepackage{graphicx} % allow embedded images
  \setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
  \graphicspath{{graphics/}} % set of paths to search for images
\usepackage{amsmath}  % extended mathematics
\usepackage{booktabs} % book-quality tables
\usepackage{units}    % non-stacked fractions and better unit spacing
\usepackage{multicol} % multiple column layout facilities
\usepackage{lipsum}   % filler text
\usepackage{fancyvrb} % extended verbatim environments
\usepackage[ngerman]{babel}
  \fvset{fontsize=\normalsize}% default font size for fancy-verbatim environments

% Standardize command font styles and environments
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment

% ================ BEISPIELE ===================

\usepackage{amsthm}
\usepackage{xcolor}

\newtheoremstyle{own}%
    {3pt}% Space above
    {3pt}% Space below
    {}% Body font
    {}% Indent amount
    {\textbf}% Theorem head font
    {:}% Punctuation after theorem head
    {.5em}% Space after theorem head
    {}% Theorem head spec

\theoremstyle{own}
\newtheorem{example}{Beispiel}[]

\begin{document}

\maketitle% this prints the handout title, author, and date

\begin{abstract}
\noindent
Notizen zur Vorlesung MA3 im Studiengang IB an der Hochschule Mannheim. Gehalten im Wintersemester 2022/23.
\end{abstract}

{\small\tableofcontents}

%\printclassoptions

\section{Kombinatorik}

\section{Verteilungen}

Eine Verteilungsfunktion hat den Aufbau:

\begin{align}
F = & P(X \leq x) \label{defver:1} \\ 
F  :&  \mathbb{R} \mapsto [0;1] \label{defver:2}
\end{align}

$F$ bildet also die Wahrscheinlichkeit für alle Werte der Zufallsvariablen $X$ für $X \leq x$ ab \eqref{defver:1}. Dabei wird für jedes $x$ eine Wahrscheinlichkeit abgebildet.

\section{Diskrete Verteilungen}

\subsection{Bernoulliverteilung}

Die Bernoulli-Verteilung ist eine \textit{diskrete} Verteilung, deren Zufallsvariable $X$ nur zwei Werte annimmt: Erfolg ($X=1$) und Misserfolg ($X=0$).

Für die Wahrscheinlichkeitsverteilung gilt:

\begin{equation}
	P(X=x) = p^x (1-p)^{1-x}.
\end{equation}

\begin{example}
	Ein Münzwurf, bei dem Kopf z.B. 1 und Zahl 0 zugeordnet wird, ist
	ein Beispiel für ein Bernoulli-Experiment. Bei einer fairen Münze ist 
	$p= \frac{1}{2}$.
\end{example}

\subsection{Binomialverteilung}

\newthought{Eine Folge von Bernoulli-Versuchen} mit der Länge $n$ und der Trefferwahrscheinlichkeit
$p$ nennen wir \textit{Bernoulli-Kette}. Diese ist binomialverteilt mit der 
Washrscheinlichkeit:

\begin{equation}
	B(n,p,k) = \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}.
\end{equation}

Der Erwartungswert der Binomialverteilung berechnet sich durch:

\begin{equation}
	\lambda = n \cdot p
\end{equation}

Die Varianz der Binomialverteilung berechnet sich wie folgt:

\begin{equation}
	\sigma^2 = Var(X) = n \cdot p \cdot (1-p).
\end{equation}

Eine Binomialverteilung mit den Parametern $n$ und $p$ lässt sich durch eine Normalverteilung annähern,
falls gilt (\textit{Laplace-Bedingung}): 

\begin{equation}
\sqrt{n \cdot p \cdot (1-p)} > 3.
\end{equation}

Bei einer Bernoulli-Kette ist es oftmals spannend zu wissen, wie hoch die
Wahrscheinlichkeit ist, dass der erste Treffer nach $k$ Versuchen auftritt.
Die Verteilungsfunktion hierzu bezeichnen wir als \textit{geometrische Verteilung}:

\begin{equation}
	P(k) = (1 - p)^{k-1} p
\end{equation}

Wenn die Wahrscheinlichkeit $P$ eines Treffers bei wachsendem $n$ gegen 0
geht\footnote{$\text{lim}_{n \rightarrow \infty} n = 0$.} und gleichzeitig
$\text{lim}_{n \rightarrow \infty} = n \cdot p = \lambda > 0$ gilt, lassen
sich die binomialverteilten Wahrscheinlichkeiten bei wachsemden $n$ mit der
\textit{Poisson-Verteilung} approximieren:

\begin{equation}
	P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!} \hspace{1em} k = 0,1,.. \hspace{1em} \lambda > 0
\end{equation}

\section{Stetige Verteilungen}

Die Dichtefunktion\footnote{Die Dichtefunktion ist das \textit{stetige} Gegenstück zur \textit{diskreten} Verteilungsfunktion}
einer stetigen Verteilung zeichnet sich, wie der Name impliziert durch das Vorhandensein eines Wert an allen Stellen aus
\footnote{Der Definitionsbereich ist $\mathbb{R}$.}. 
Wir nutzen hier anstelle einer Verteilungsfunktion eine Dichtefunktion
\footnote{Das Integral bis $x$ stellt die Wahrscheinlichkeit $P(X \leq x)$ dar.}:

\begin{equation}
	\int_{-\infty}^x f(t) dt 
\end{equation}

Damit es sich um eine Dichtefunktion einer Verteilung handelt, müssen zwei Eigenschaften erfüllt sein:
\marginnote{

\eqref{dichteeig:1}: Wahrscheinlichkeiten können nicht negativ sein

\eqref{dichteeig:2}: Insgesamt beträgt die kumulierte Wahrscheinlichkeit 1
}
\begin{align}
&f(t) \geq 0 \hspace{1em} \forall t \in \mathbb{R} \label{dichteeig:1}\\
&\int_{-\infty}^{\infty} f(t) dt =1 \label{dichteeig:2}
\end{align}

\subsection{Gleichverteilung}

Bei einer Gleichverteilung definieren wir die Dichtefunktion:

\begin{equation}
f(x) = \begin{cases}
\frac{1}{b-a} \hspace{1em} &\text{für} \hspace{1em} x \in [a,b] \\
0 \hspace{1em} &\text{sonst}
\end{cases}
\end{equation}

\begin{marginfigure}
	\includegraphics{gleichverteilung}
	\caption{Beispiel einer Gleichverteilung}
\end{marginfigure}

Die Wahrscheinlichkeiten berechnen sich:

\begin{equation}
P(X\leq x) = F(X) = \begin{cases}
0 \hspace{1em} & \text{für}  \hspace{1em} x < a \\
\frac{x-a}{b-a} \hspace{1em} & \text{für}  \hspace{1em} a \leq x \leq b \\
1 \hspace{1em} & \text{für}  \hspace{1em} x > b
\end{cases}
\end{equation}

\subsection{Exponentielle Verteilung}

Die Dichtefunktion einer exponentiellen Verteilung hat die Form:

\begin{equation}
	f(x) = \lambda \cdot e^{-\lambda x}
\end{equation}

Nach Bedingung \eqref{dichteeig:2} müssen wir zeigen, dass
$\int_{-\infty}^\infty = 1$.\footnote{Da wir mit einer Exponentialfunktion arbeiten, die wir für $x\geq0$ definieren, beweisen wir analog $\int_0^\infty = 1$.}

\begin{proof}
\begin{align*}
\int_0^\infty f(t) dt &= \lambda \int_0^\infty e^{-\lambda t} dt = \lambda \cdot [\frac{e^{-\lambda t}}{- \lambda}]_0^\infty \\
	&= [-e^{-\lambda t}]_0^\infty = -0 - (-1) = 1
\end{align*}
\end{proof}

\begin{marginfigure}
	\includegraphics{exponentialverteilung}
	\caption{Beispiel einer Exponentialverteilung}
\end{marginfigure}

Die entsprechende Wahrscheinlichkeitsfunktion:

\begin{equation}
F(x) = \begin{cases}
1 - e^{- \lambda x} \hspace{1em} & \text{für} \hspace{1em} x >0 \\
0 \hspace{1em} & \text{für} \hspace{1em} x \leq 0
\end{cases}
\end{equation}

\subsection{Normalverteilung}

Die Normalverteilung folgt der grundlegenden Form $f(t) = e^{- t^2}$. Überprüfen wir zunächst
die Eigenschaften \eqref{dichteeig:1}\footnote{trivial, $e^0 = 1$} und \eqref{dichteeig:2}.

\begin{marginfigure}
	\includegraphics{normalverteilung}
	\caption{Beispiel einer Normalverteilung}
\end{marginfigure}

Zu \eqref{dichteeig:2}:

\begin{gather*}
	\int_{-\infty}^\infty e^{\frac{- t^2}{2}} dt = \sqrt{2 \Pi}
\end{gather*}

Da $\sqrt{2\Pi} > 1$, ist Bedingung \eqref{dichteeig:2} nicht erfüllt. Wir definieren daher die \textit{Standard-Normalverteilung} $\phi$ wie folgt
\footnote{Wir normieren die Funktion auf das berechnete Integral, damit $\int_{-\infty}^\infty = 1$}:

\begin{equation}
\phi(t) = \frac{e^{\frac{-t^2}{2}}}{\sqrt{2 \Pi}}
\end{equation}

Zusätzlich definieren wir die \textit{allgemeine Normalverteilung}
\footnote{
$\mu$: Verschiebung entlang der x-Achse \\
$\sigma$: Stauchung/Streckung}:

\begin{equation}
\phi(x) = \frac{1}{\sqrt{2 \Pi} \sigma} \cdot e^{- \frac{1}{2} (\frac{x - \mu}{\sigma})^2}
\end{equation}

Die Wahrscheinlichkeitsverteilung berechnet sich durch\footnote{Die Wahrscheinlichkeitsverteilung zu einer Dichtefunktion ist einfach das Integral}:

\begin{equation}
\Phi(x) = \frac{1}{\sqrt{2 \Pi} \sigma} \int_{-\infty}^x e^{- \frac{1}{2} (\frac{t - \mu}{\sigma})^2} dt
\end{equation}

Hierbei wird ein uneigentliches Integral\footnote{Integral mit $\infty$ als Grenze, Wert nicht einfach berechenbar} verwendet, dessen Berechnung nur über Näherungsverfahren möglich ist. 
Hierfür gibt es Tabellen, die in der Klausur gestellt werden.

\subsection{Unabhängigkeit zweier Zufallsvariablen}

Sei $(\Omega, P)$ ein Wahrscheinlichkeitsraum und $X,Y : \Omega \mapsto \mathbb{R}$ zwei Zufallsgrößen.
$X$ und $Y$ heißen \textit{unabhängig}, wenn $\forall x \in W(X), y \in W(Y)$:

\begin{equation}
	P(X=x, Y=y) = P(X=x) \cdot P(Y=y)
\end{equation}

gilt. Bei gemeinsamen Verteilungen sprechen wir von \textit{diskreten} Zufallsvariablen.
Die Unabhängigkeit lässt sich zeigen durch eine Vierfeldertafel, bei der alle
Werte durch Multiplikation berechnet werden und eine Addition der Einzelwahrscheinlichkeiten
1 ergibt (siehe Beispiel \ref{bsp:unabhängig}). Die Abhängigkeit lässt sich 
durch ein Gegenbeispiel beweisen.

\begin{marginfigure}
  \includegraphics{zweizufallsvariablen}
  \caption{Mögliche Kombinationen zweier Zufallsvariablen in $\Omega$.}
\end{marginfigure}

\begin{example}\label{bsp:unabhängig}
Beispiel einer unabhängigen, gemeinsamen Verteilung.
Hier sehen wir das
an der Vierfeldertafel\footnote{Siehe Beispielsweise: $P(X=0,Y=0) = P(X) \cdot P(Y) = \frac{1}{4} \cdot \frac{1}{4} = \frac{1}{16}$}:

\begin{center}
\begin{tabular}{l|lll}
 X/Y&  Y=0 &  Y=1  &   \\
\hline
 X=0& $\frac{1}{16}$ & $\frac{3}{16}$ & 0.25  \\
X=1 & $\frac{3}{16}$  &  $\frac{9}{16}$ &  0.75  \\
 &  0.25 & 0.75  & 1 
\end{tabular}
\end{center}
\end{example}

Die Wahrscheinlichkeitsverteilung einer diskreten 2-dimensionalen
Zufallsgröße lässt sich durch die Wahrscheinlichkeitsfunktion

\begin{equation}
f(x) = \begin{cases}
P_{ik} \hspace{1em} \text{falls}  \hspace{1em} x=x_i, y =y_k \\
0 \hspace{1em} \text{sonst}
\end{cases}
\end{equation}

darstellen.

\section{Erwartungswert}

Für eine \textit{diskrete} Zufallsgröße ist der Erwartungswert

\begin{equation}
E(X) = \sum_i x_i P(X=x_i).
\end{equation}

\begin{example}\label{bsp:erwartungswertdiskret}
 Erwartungswert einer gegebenen Wahrscheinlichkeitsverteilung


\begin{center}
\begin{tabular}{l|llllll}
 $x_i$&  0 & 2 & 3 & 4 & 10 & 11  \\ \hline
 $p_i$&  $\frac{3}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$
\end{tabular}
\end{center}

\begin{gather*}
E(X) = 0 \cdot \frac{3}{8} + 2 \cdot \frac{1}{8} + 3 \cdot \frac{1}{8} + 4 \cdot \frac{1}{8} + 10 \cdot \frac{1}{8} + 11 \cdot \frac{1}{8}
\end{gather*}

\end{example}

Für eine \textit{stetige} Zufallsgröße ist der Erwartungswert
\footnote{Funktioniert prinzipiell genau wie im \textit{diskreten}, aber mit Integral statt Summe}:

\begin{equation}
E(X) = \int_{-\infty}^{\infty} y \cdot f(y) dy
\end{equation}

\begin{example}\label{bsp:erwartungswertstetig}
Erwartungswert einer stetigen Verteilung\footnote{gerne in Klausuren gefragt}

Gegeben eine Dichtefunktion $f$:

\begin{equation*}
	f(x) = \begin{cases} 
	ax^2, 0 \leq x \leq 5 \\
	0, \text{sonst}
	\end{cases}
\end{equation*}

Wir berechnen zuerst $a$, sodass die Bedingungen \eqref{dichteeig:1} und
\eqref{dichteeig:2} erfüllt sind:

\begin{gather*}
\int_{-\infty}^{\infty} f(x)dx = \int_0^5 ax^2 dx = 1 \\
[\frac{a x^3}{3}]_0^5 = \frac{125a}{3} - \frac{0a}{3} =  \frac{125a}{3} = 1 \\
a = \frac{3}{125}
\end{gather*}

Der Erwartungswert berechnet sich nun wie folgt:

\begin{gather*}
E(X) = \int_0^5 x \frac{3}{125} x^2 dx \\
= [\frac{3}{125} \cdot \frac{x^4}{4}]_0^5 = \frac{3}{125} \cdot {625}{4} = \frac{15}{4}
\end{gather*}

\end{example}

\subsection{Rechenregeln für den Erwartungswert}

Sei $(\Omega, P)$ ein Wahrscheinlichkeitsraum und $X,Y : \Omega \mapsto \mathbb{R}$ zwei Zufallsgrößen. Es gilt:

\begin{gather}
E(X + Y) = E(X) + E(Y) \\
E( a \cdot X + b \cdot Y) = a \cdot E(X) + b \cdot E(Y)  \\
\text{Sind X , Y unabhängig dann gilt} \hspace{1em} E (X \cdot Y ) = E ( X )\cdot E ( Y )
\end{gather}

\section{Varianz}

Der mittlere Abstand der Werte vom Erwartungswert. Die quadratische Funktion
ist besser als der Betrag analytisch, weshalb wir diese verwenden.

Die Varianz zum Beispiel \autoref{bsp:erwartungswertdiskret}:

\begin{gather*}
Var(X) = - \frac{15}{4} ^2 \frac{3}{8} + \frac{-7}{4}^2 \frac{1}{8} + \frac{-3}{4}^2 \frac{1}{8} + \frac{1}{4}^2 \frac{1}{8} + \frac{25}{4}^2 \frac{1}{8} + \frac{28}{4}^2 \frac{1}{8}
\end{gather*}

Die Varianz zum Beispiel \ref{bsp:erwartungswertstetig}:

\begin{gather*}
	f(x) = \begin{cases} 
	\frac{3}{125}x^2, 0 \leq x \leq 5 \\
	0, \text{sonst}
	\end{cases} \\
	E(X) = \frac{15}{4} \\
	Var(X) = \int_{-\infty}^\infty (x - \frac{15}{4})^2 f(x) dx \\
	= \int_0^5 (x - \frac{15}{4})^2 \frac{3}{125} x^2 dx
\end{gather*}

Die Varianz lässt sich so (einfacher) auch so berechnen:\footnote{Das ist nicht besser, aber einfacher. Nicht die schöne Definition, sondern den schnellen Weg wählen.}

\begin{equation}
\label{var:einfach}
Var(X) = E(X^2) - (E(X))^2 
\end{equation}

\begin{proof}
\begin{align*}
Var(X) :&= E((X - E(X))^2) \\
&= E(X^2 - 2E(X) \cdot X  + (E(X))^2) \\
&= E(X^2) - E(2E(X)\cdot X) + E((E(X))^2) \\
&= E(X^2) - E(X)E(X) + (E(X))^2 \\
&= E(X^2) - (E(X))^2
\end{align*}
\end{proof}

\marginnote{
\begin{tabular}{l|llllll}
 $x_i$&  0 & 2 & 3 & 4 & 10 & 11  \\ \hline
 $p_i$&  $\frac{3}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ \\ \hline
 $x_i^2$& 0 & 4 & 9 & 16 & 100 & 121
\end{tabular}

aktualisierte Tabelle zu Beispiel \ref{bsp:erwartungswertdiskret}
}


Die Varianz zum Beispiel \autoref{bsp:erwartungswertdiskret}:

\begin{gather*}
Var(X) = \frac{250}{8} - \frac{15^2}{4^2} = \frac{500-225}{16} = \frac{225}{16}
\end{gather*}

Die Varianz zum Beispiel \autoref{bsp:erwartungswertstetig}:

\begin{gather*}
E(X^2) = \int_{-\infty}^\infty x^2 f(x) dx = \frac{3}{125} \int_0^5 x^2 x^2 dx \\
= \frac{3}{125} [\frac{x^5}{5}]_0^5 = 15.
\end{gather*}

\begin{gather*}
Var(X) = 15 - (\frac{15}{4})^2 = \frac{15 \cdot 16 - 15^2}{16} = \frac{15}{16}.
\end{gather*}

\subsection{Die Ungleichung von Tschebyscheff}

$M = W(A) \cap (]-\infty; E(A) - a) \cup (E(A)+a; +\infty [)$

$Var(A) := \sum_{i=1}^k (x_i - E(A))^2 \cdot P(A = x_i) \ge \sum_{x_i \in M}
(x_i - E(A))^2 P(A=x_i) \ge \sum_{x_i \in M} a^2 P(A=x_i) = a^2 \sum_{x_i \in M}
P(A = x_i) = P(|A - E(A)| \ge a)$

Anwenden bei: symmetrischem Intervall um Erwartungswert herum, Erwartungswert
und Varianz müssen vorhanden sein. Frage: im symmetrischen Intervall oder
außerhalb des symmetrischen Intervalls.

\begin{example}
	Obsthändler, Binomialverteilung, $n=240$, $p=\frac{1}{20}$.
	Gesucht $P(6 \le X \le 18)$?
	$E(X) = np = 240 \cdot \frac{1}{20} = 12$
	$Var(X) = 12 \cdot {19}{20} = \frac{57}{5} \approx 11.4$
	Das Intervall ist geeignet für Tschebyscheff, weil $E(X)$ genau in der Mitte
	des Intervalls liegt.
	$P(6 \le X \le 18) = 1 - P(|X - 12| \ge 7)$
	$\ge 1 - \frac{11.4}{7^2} \approx 1 - 0.2326 = 0.7673$.
	Es liegen \emph{mindestens} $76\%$ im Intervall.
\end{example}

\subsection{Satz}

Sei $X_1,...,X_n$ eine Folge identisch verteilter unabhäniger Zufallsgrößen
mit $E(X_i) = \mu$ und $Var(X_i) = \sigma^2 \forall i \in \mathbb{N}$,

\begin{equation}
	U_n = \frac{(X_1 + ... + X_n) - n\mu}{\sqrt{n} \cdot \sigma}.
\end{equation}

Dann konvergieren die Verteilungsfunktionen $F_n$ der Zufallsgrößen $U_n$ gegen
die Standardnormalverteilung.

\begin{example}
	\begin{gather*}
		P(6 \le X \le 18) = P(\frac{6-12}{\sqrt{11.4}} \le \frac{X-12}{\sqrt{11.4}} 
		\le \frac{18-12}{\sqrt{11.4}}\\
		= \Phi (\frac{6}{\sqrt{11.4}}) - \Phi (\frac{-6}{\sqrt{11.4}}) \\
		= 2 \Phi (\frac{6}{\sqrt{11.4}}) - 1 \approx 0.9232
	\end{gather*}
	Hinweis: $P(6 \le X \le 18) = P(5 < X <19)$.
\end{example}

Wenn die Varianz nicht zu groß ist, kann ein Korrekturfaktor ($0.5$) bei der
Berechnung der Approximation durch die Standardnormalverteilung aufaddiert
werden (Siehe Folie 94).

\subsection{Bernoullisches Gesetz der großen Zahlen}

Satz: Sei $n \in \mathbb{N}, 0 < p < 1, v_n = \frac{A}{n}$ die relative 
Häufigkeit eines Ereignisses $A$ in einer Bernoullikette der Länge $n$. Es 
gelte außerdem $P(A) = p$. Dann gilt für jedes $\epsilon > 0$

\begin{equation}
	\lim_{n \rightarrow +\infty} P(|v_n - p| > \epsilon) = 0.
\end{equation}

$\epsilon$ ist die Ungenauigkeit.

\begin{example}
Bernoullikette der Länge $n$ mit der Trefferwahrscheinlichkeit $p$, $A$
binomialverteilt, neue Zufallsgröße: $\frac{A}{n}$

$E(\frac{A}{n}) = \frac{1}{n} E(A) = \frac{1}{n} \cdot n \cdot p = p$
$Var(\frac{A}{n}) = \frac{1}{n^2}$i

$P(|\frac{A}{n} - p| > \epsilon) \le \frac{p \cdot (1 - p)}{n \cdot \epsilon^2}$
Geht für $n$ gegen $\infty$ gegen 0.
\end{example}

\begin{example}
	Eine Münze wird 10000 mal geworfen, Wahrscheinlichkeit für 4990 bis 5010 mal
	Kopf.$n=10000, p=\frac{1}{2}, E(X) = 5000, Var(X) = 2500$
	
	\begin{gather}
		P(4990 \le X \le 5010) = 1 - P(|X - 5000| \ge 11)\\
		= 1 - \frac{2500}{11^2} = 1 - \frac{2500}{121} \approx -19.6
	\end{gather}
	
	Nach der Ungleichung von Tschebyscheff wissen wir, dass die Wahrscheinlichkeit
	größer als $-19.6$ ist, korrekt aber naja.
	
	Mit der Standardnormalverteilung:
	
	\begin{gather*}
	P(\frac{4990 - 5000}{50} \le \frac{X - 5000}{50} \le \frac{5010}{50})\\
	= P(- \frac{1}{5} \le T \le \frac{1}{5}) \\
	\approx \Phi(\frac{1}{5}) - \Phi(-\frac{1}{5}) \\
	=2 \Phi(\frac{1}{5}) - 5 \approx 0.1586
	\end{gather*}
\end{example}

\subsection{Die Kovarianz von zwei Zufallsgrößen}

Die \emph{Kovarianz} von zwei Zufallsgrößen $X$ und $Y$ wird durch

\begin{equation}
	cov(X,Y) = E((X - E(X)))(Y - E(Y))
\end{equation}

Wenn die Kovarianz nicht 0 ist, sind die Variablen \emph{immer} abhängig.

%=============================================================================

\section{Statistik}

Methodensammlung zum Umgang mit Wahrscheinlichkeiten/Zahlen in der realen Welt.

\emph{Grundgesamtheit}: Gesamtheit gleichartiger Objekte, die hinsichtlich eines
bestimmten Merkmals untersucht werden sollen.

Eine \emph{Stichprobe} vom Umfang $n \in \mathbb{N}$ ist eine Folge $X_1,...,X_n$ von
unabhängigen identisch verteilten Zufallsgrößen. Eine \emph{Statistik} ist eine
Zufallsgröße

\begin{equation}
	g(X_1,...,X_n) \text{ mit } g: \mathbb{R}^n \mapsto \mathbb{R}.
\end{equation}

Der \emph{Modus} ist der häufigste Wert, der in der Stichprobe vorkommt.

Der \emph{Median} ist der Wert, der genau in der Mitte der Datenverteilung liegt.
Bei einer geraden Anzahl von Individualdaten ist die Hälfte der Summe der beiden
in der Mitte liegenden Werte.

\begin{example} Altersverteilung Studierende

Stichprobe: 23, 25, 22, 23, 36, 23, 20, 18, 19, 20, 24

Modus: 23, da der Wert dreimal vorkommt.

Geordnet (hervorg: Median): 18, 19, 20, 20, 22, \emph{23}, 23, 23, 24, 25, 26

Mittelwert: 23

\end{example}

\subsection{Erwartungstreuer Schätzer für den Erwartungswert}

Sei $(X_1,...,X_n)$ eine Stichprobe vom Umfang $n$ mit $E(X_i) = \mu \in \mathbb{R}$
und $Var(X_i) = \sigma^2$ für alle $i \in \{1,...,n\}$ und

\begin{equation*}
	\Bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\end{equation*}

Es gilt:

\begin{align}
&E(\Bar{X}) = \mu \\
&Var(\Bar{X}) = \frac{\sigma^2}{n} \label{eq:varianz}
\end{align}

Man sagt, $\Bar{X}$ ist ein erwartungstreuer Schätzer für $\mu$.

Herleitung für \eqref{eq:varianz}: $Var(\Bar{X}) = (\frac{1}{n})^2 \sum_{i=1}^n Var(X_i)
 = \frac{1}{n^2} \cdot n \cdot \sigma^2 = \frac{\sigma^2}{n}$.

\subsection{Varianz einer Stichprobe}

Sei $(X_1,..X_n)$ eine Stichprobe vom Umfang $n$ mit $E(X_i) = \mu \in \mathbb{R}$
und $Var(X_i) = \sigma^2 in \mathbb{R}^{+}$ für alle $i \in 1,...,n$ und

\begin{equation}
	S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \Bar{X})^2
\end{equation}

mit $\Bar{X} = $


Herteilung einer alternativen Formel für die Varianz
\begin{gather}
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \Bar{X})^2
= \frac{1}{n-1} \sum_{i=1}^n (X_i^2 - 2\Bar{X}X_i + (\Bar{X})^2 ) \\
= \frac{1}{n-1} \sum_{i=1}^n X_i^2 - 2\Bar{X} \sum_{i=1}^n X_i + n (\Bar{X})^2 \\
= \frac{1}{n-1} \sum_{i=1}^n X_i^2 - 2 (\Bar{X})^2 \cdot n + n (\Bar{X})^2 \\
= \frac{1}{n-1} (\sum_{i=1}^n X_i^2 - n \cdot (\Bar{X})^2 )
\end{gather}

\begin{example} Kennzahlen für Lineare Regression berechnen

1) $\Bar{X}, \Bar{Y}$

2)

3)

\end{example}

\bibliography{sample-handout}
\bibliographystyle{plainnat}



\end{document}
